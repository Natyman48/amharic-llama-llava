# amharic-llama-llava

Pretraining and finetuning scripts for Amharic LLaMA adapted from:

https://github.com/ymcui/Chinese-LLaMA-Alpaca

https://github.com/facebookresearch/llama-recipes

Llama-2-Amharic weights: https://huggingface.co/iocuydi/llama-2-amharic-3784m
Can be run with the inference script in this repo. Pretrained on 3.784b Amharic tokens.

Additional training, eval, translation, data processing, and LLaVA-related scripts/models to be released, along with datasets

https://medium.com/@garrilogistics/llama-2-amharic-llms-for-low-resource-languages-d6fb0ba332f4

Cite: 
```
@misc{andersland2024amharic,
      title={Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages}, 
      author={Michael Andersland},
      year={2024},
      eprint={2403.06354},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
